{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross entropy on the Softmax classifier minimizes the distance between the estimated class probabilities and the true distribution, which in this interpretation is the distribution where all probability mass is on the true/correct classes. This can also be described as minimizing the negative log likelihood of the correct class, which is essentially performing Maximum Likelihood Estimation. In summary, the cross entropy metric steers the distribution of the estimates toward that of the true distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary logistic regression classifier has only two classes (0,1), while a multi-class logistic regression classifier can categorize input into one of several classes. In the binary logistic regression setting, the probability that a particular observation falls in class 1  P(y=1|x, w) is given by sigmoid(f) where f = W*X_i. To generalize this to a multi-class regression, we can replace P(y=1|x,w) with P(y=i|x,w), which is calculated using the softmax function: $$Softmax= \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}.$$\n",
    "\n",
    "In a deep learning classification model, we seek to find parameters (weights and bias) for each layer that will lead us to a local minimum of the cost function. First the loss function is calculated based on some initial values for the parameters, then the gradients of the loss function with respect to the parameters are calcualted using back propogation. Then, an optimizer like stochastic gradient descent is used to shift the parameters in order to decrease the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU function is defined as max(0, a). One of the advantages of ReLU is that it does not activate all neurons at the same time. This makes the ReLU more efficient than certain other activation functions. Another advantage is that the gradient is constant for a>0, which is less likely to suffer from vanishing gradients. For comparison, the sigmoid activation will have very small gradients for when X is large in absolute terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is done by setting aside a hold out set and training the model on the remaining data. Afterwards, the model is tested on the hold out set in order to get an estimate of the model's true error rate. In K-fold cross validadtion, the data is split into k groups. For each group k_i, we use the rest of the data as training data and evaluate the loss or error rate on k_i. Then we average the k error rates found to arrive at our estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began with the default parameters given by the example as a starting point, and obtained a test accuracy of around 41%. I then experimented with decreasing and increasing the batch size while keeping other parameters constant, and found that a smaller batch size of around 300 led to an improvement of my accuracy to around 43%. Similar experimentation with the learning decay, regularization, and number of epochs lead me to select 0.93 for learning decay, 1 for regularization, and 15 for number of epochs. This tuning yielded around a 1% increase in the accuracy. \n",
    "\n",
    "At this point I altered my Stochastic Gradient Descent optimizer to also use momentum and repeated the process above. Experimentation with the momentum lead me to a final model with the hyperparameters below. \n",
    "\n",
    "\n",
    "reg=1\n",
    "weight_scale=1e-3\n",
    "num_epoch = 15\n",
    "batch_size = 300\n",
    "lr = 1e-3\n",
    "learning_decay = 0.93\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity parameter seemed to perform reasonably well for values between 15 and 50. The clusters appeared to become more blurred out the higher the perplexity, so I expect larger datasets to require higher values for the perplexity parameter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
